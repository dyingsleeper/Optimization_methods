{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJIYYz8BoS07"
   },
   "source": [
    "# <center>Лабораторная работа 2.</center>\n",
    "## <center>Многомерная безусловная оптимизация</center>\n",
    "\n",
    "*Автор материала: к.т.н., доцент кафедры Фундаметальной информатики и оптимального управления ВолГУ Михаил Алексеевич Харитонов*\n",
    "\n",
    "**Цель работы:** Приобретение практических навыков реализации методов поиска безусловного экстремума в языке Python с использованием Jupyter Notebook.\n",
    "\n",
    "**Задание:** Заполните ответ в клетках (где написано \"Ваш код здесь\" или \"Ваш ответ здесь\"), ответьте на вопросы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDfDRGZ7_zd-"
   },
   "source": [
    "# Часть 1 Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En5AiGj8Ja2k"
   },
   "source": [
    "## 1.1 Метод нулевого порядка (метод Хука-Дживса)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2vXCeiu50BF"
   },
   "source": [
    "\n",
    "**Шаг 1.** Задать начальную точку $x^0$,число $\\varepsilon >0$ для остановки алгоритма, начальные величины шагов по координатным направлениям $\\Delta_1,\\ldots, \\Delta_n>\\varepsilon$, ускоряющий множитель       $\\lambda>0$, коэффициент уменьшения шага $\\alpha > 1$. Положить $y^1=x^0$, $i=1$, $k=0$.\n",
    "\n",
    "**Шаг 2.** Осуществить исследующий поиск по выбранному координатному направлению:\n",
    "\n",
    "  a) если  $f(y^i+\\Delta_i d_i)<f(y^i)$, шаг считается удачным. В этом случае следует положить $y^{i+1}=y^i+\\Delta_i d_i$ и перейти к шагу 3.\n",
    "\n",
    "  b) если в п. a шаг неудачен, то делается шаг в противоположном направлении. Если $f(y^i-\\Delta_i d_i)<f(y^i)$, шаг считается удачным. В этом случае следует положить  $y^{i+1}=y^i-\\Delta_i d_i$ и перейти  к шагу 3.\n",
    "\n",
    "  c) если в пп. a и b шаги неудачны, положить $y^{i+1}=y^i$.\n",
    "\n",
    "**Шаг 3.** Проверить условия:\n",
    "\n",
    "  a) если $i<n$, то положить $i=i+1$ и перейти к шагу 2 (продолжить исследующий поиск по оставшимся направлениям);\n",
    "  b) если $i=n$, проверить успешность исследующего поиска:\n",
    "\n",
    "   - если $f(y^{n+1})<f(x^k)$, перейти к шагу 4.\n",
    "\n",
    "   - если $f(y^{n+1})\\geq f(x^k)$, перейти к шагу 5.\n",
    "\n",
    "**Шаг 4.** Провести поиск по образцу. Положить $x^{k+1}=y^{n+1}$, $y^1=x^{k+1}+\\lambda(x^{k+1}-x^{k}),i=1,k=k+1$ и перейти к шагу 2.\n",
    "\n",
    "**Шаг 5.** Проверить условие окончания:\n",
    "\n",
    "  a) если все $\\Delta_i\\leq\\varepsilon_i$, то поиск закончить: $x^* \\cong x^k$.\n",
    "\n",
    "  b) для тех $i$, для которых $\\Delta_i>\\varepsilon_i$, уменьшить величину шага: $\\Delta_i=\\frac{\\Delta_i}{\\alpha}$ .\n",
    "            Подожить $y^1=x^k$, $x^{k+1}=x^k$, $k=k+1, i=1$ и перейти к шагу 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTio8eoeLVBH"
   },
   "source": [
    "### Задание 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVeM5Lk9La4-"
   },
   "source": [
    "1) Напишите функцию реализующую алгоритм поиска минимима функции методом метод Хужа-Дживса.  \n",
    "2) С помощью функции найдите минимальное значение функции $$f_1(x)=x_1^3-x_1x_2+x^2_2-2x_1+3x_2-4.$$\n",
    "3) С помощью функции найдите минимальное значение функции Розенброка $$f_2(x)=(1-x_1)^{2}+100(x_2-x_1^{2})^{2}.$$\n",
    "\n",
    "4) С помощью функции найдите максимум функции $$f_3(x)=\\frac{10}{30(x_2-x_1^2)^2+5(1.5+x_1)^2+1}.$$\n",
    "\n",
    "5)  С помощью функции найдите минимальное значение функции  $$f_4(x)=x_1^2+5x_2^2+3x_3^2+4x_1x_2-2x_2x_3-2x_1x_3.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XwauhReQe3RM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hooke_jeeves(func, init_point, epsilon=1e-6, step_sizes=None, delta=2.0, alpha=2.0, max_iterations=10000):\n",
    "    \"\"\"\n",
    "    Реализация метода Хука-Дживса для многомерной оптимизации.\n",
    "\n",
    "    Параметры:\n",
    "        func (callable): Оптимизируемая функция.\n",
    "        init_point (array-like): Начальная точка поиска минимума.\n",
    "        epsilon (float): Точность, при которой поиск прекращается.\n",
    "        step_sizes (array-like): Начальные шаги для каждого измерения. Если None, устанавливается 0.5 для всех измерений.\n",
    "        delta (float): Коэффициент масштабирования при движении в направлении уменьшения функции.\n",
    "        alpha (float): Коэффициент уменьшения шага, если прогресса нет.\n",
    "        max_iterations (int): Максимальное число итераций алгоритма.\n",
    "\n",
    "    Возвращает:\n",
    "        tuple: Точка минимума, значение функции в этой точке, количество выполненных итераций.\n",
    "    \"\"\"\n",
    "    if step_sizes is None:\n",
    "        step_sizes = np.full_like(init_point, 0.5)  # Устанавливаем шаги по умолчанию\n",
    "\n",
    "    current_point = np.array(init_point, dtype=float)  # Текущая точка оптимизации\n",
    "    iter_count = 0\n",
    "\n",
    "    while iter_count < max_iterations:\n",
    "        exploratory_point = np.array(current_point)  # Точка для исследования\n",
    "        \n",
    "        for i in range(len(current_point)):\n",
    "            trial_point_plus = exploratory_point.copy()\n",
    "            trial_point_plus[i] += step_sizes[i]  # Пробуем шаг вперёд\n",
    "            \n",
    "            if func(trial_point_plus) < func(exploratory_point):\n",
    "                exploratory_point[i] += step_sizes[i]  # Если улучшение, сохраняем шаг\n",
    "            else:\n",
    "                trial_point_minus = exploratory_point.copy()\n",
    "                trial_point_minus[i] -= step_sizes[i]  # Пробуем шаг назад\n",
    "                if func(trial_point_minus) < func(exploratory_point):\n",
    "                    exploratory_point[i] -= step_sizes[i]  # Если улучшение, сохраняем шаг назад\n",
    "\n",
    "        if func(exploratory_point) < func(current_point):\n",
    "            search_direction = exploratory_point - current_point  # Направление движения\n",
    "            new_point = exploratory_point + delta * search_direction  # Новый базисный шаг\n",
    "            \n",
    "            if func(new_point) < func(exploratory_point):\n",
    "                current_point = new_point  # Принятие нового базиса\n",
    "            else:\n",
    "                current_point = exploratory_point\n",
    "        else:\n",
    "            step_sizes /= alpha  # Уменьшаем шаги\n",
    "            if np.all(step_sizes <= epsilon):\n",
    "                break  # Условие завершения\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "    return current_point, func(current_point), iter_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для функции f1:\n",
      "Точка минимума: x = [ 0.5  -1.25]\n",
      "Значение функции в точке минимума: f(x) = -6.4375\n",
      "Количество итераций: 22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x[0] ** 3 - x[0] * x[1] + x[1] ** 2 - 2 * x[0] + 3 * x[1] - 4\n",
    "\n",
    "init_point_f1 = np.array([3.0, -2.0])\n",
    "minimum_f1, min_value_f1, iterations_f1 = hooke_jeeves(f1, init_point_f1)\n",
    "\n",
    "print(\"Результаты для функции f1:\")\n",
    "print(f\"Точка минимума: x = {minimum_f1}\")\n",
    "print(f\"Значение функции в точке минимума: f(x) = {min_value_f1}\")\n",
    "print(f\"Количество итераций: {iterations_f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для функции Розенброка f2:\n",
      "Точка минимума: x = [0.9996727  0.99934578]\n",
      "Значение функции в точке минимума: f(x) = 1.0713348767187923e-07\n",
      "Количество итераций: 2129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    \"\"\"Функция Розенброка\"\"\"\n",
    "    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "init_point_f2 = np.array([-1.2, 1.0])\n",
    "minimum_f2, min_value_f2, iterations_f2 = hooke_jeeves(f2, init_point_f2)\n",
    "\n",
    "print(\"Результаты для функции Розенброка f2:\")\n",
    "print(f\"Точка минимума: x = {minimum_f2}\")\n",
    "print(f\"Значение функции в точке минимума: f(x) = {min_value_f2}\")\n",
    "print(f\"Количество итераций: {iterations_f2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для функции f3 (максимум):\n",
      "Точка максимума: x = [-1.4999485  2.2498455]\n",
      "Значение функции в точке максимума: f(x) = 9.999999867395672\n",
      "Количество итераций: 653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f3(x):\n",
    "    return 10 / (30 * (x[1] - x[0] ** 2) ** 2 + 5 * (1.5 + x[0]) ** 2 + 1)\n",
    "\n",
    "def neg_f3(x):\n",
    "    \"\"\"Отрицательная версия функции f3\"\"\"\n",
    "    return -f3(x)\n",
    "\n",
    "init_point_f3 = np.array([0.0, 0.0])\n",
    "maximum_f3_point, neg_max_value_f3, iterations_f3 = hooke_jeeves(neg_f3, init_point_f3)\n",
    "maximum_f3_value = f3(maximum_f3_point)\n",
    "\n",
    "print(\"Результаты для функции f3 (максимум):\")\n",
    "print(f\"Точка максимума: x = {maximum_f3_point}\")\n",
    "print(f\"Значение функции в точке максимума: f(x) = {maximum_f3_value}\")\n",
    "print(f\"Количество итераций: {iterations_f3}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для функции f4:\n",
      "Точка минимума: x = [0. 0. 0.]\n",
      "Значение функции в точке минимума: f(x) = 0.0\n",
      "Количество итераций: 18\n"
     ]
    }
   ],
   "source": [
    "def f4(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 2 + 3 * x[2] ** 2 + 4 * x[0] * x[1] - 2 * x[1] * x[2] - 2 * x[0] * x[2]\n",
    "\n",
    "init_point_f4 = np.array([0.0, 0.0, 0.0])\n",
    "minimum_f4, min_value_f4, iterations_f4 = hooke_jeeves(f4, init_point_f4)\n",
    "\n",
    "print(\"Результаты для функции f4:\")\n",
    "print(f\"Точка минимума: x = {minimum_f4}\")\n",
    "print(f\"Значение функции в точке минимума: f(x) = {min_value_f4}\")\n",
    "print(f\"Количество итераций: {iterations_f4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjuLXbxGN8M_"
   },
   "source": [
    "## 1.2 Метод первого порядка (метод градиентного спуска с постоянным шагом)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYGz2F0oOMp4"
   },
   "source": [
    "**Шаг 1.** Задать $x^0$, $0<\\varepsilon < 1$, $\\varepsilon_1 >0$, $\\varepsilon_2>0$, $M$ предельное число итераций.\n",
    "Найти градиент функции в произвольной точке $\\nabla f(x)=\\left ( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)^T$\n",
    "\n",
    "**Шаг 2.** Положить $k=0$.\n",
    "\n",
    "**Шаг 3.** Вычислить $\\nabla f\\left(x^k\\right)$.\n",
    "\n",
    "**Шаг 4.** Проверить выполнение критерия окончания $\\left\\|\\nabla f\\left(x^k\\right)\\right\\|<\\varepsilon_1$:\n",
    "\n",
    "  - если критерий выполнен, расчет закончен, $x^*=~x^k$;\n",
    "  - если критерий не выполнен, то перейти к шагу 5.\n",
    "\n",
    "\n",
    "**Шаг 5.** Проверить выполнение неравенства $k\\ge M$:\n",
    "\n",
    "  - если неравенство выполнено, то расчет окончен:  $x^*=~x^k$;\n",
    "  - если нет, то перейти к шагу 6.\n",
    "\n",
    "**Шаг 6.**  Задать величину шага $t_k$.\n",
    "\n",
    "**Шаг 7.**  Вычислить $x^{k+1}=~x^k-~t_k~\\nabla f\\left(x^k\\right)$.\n",
    "\n",
    "**Шаг 8.**  Проверить выполнение условия $f\\left(x^{k+1}\\right)-f\\left(x^k\\right)<0$ (или $f\\left(x^{k+1}\\right)-f\\left(x^k\\right)<-\\varepsilon \\left\\|\\nabla f(x^k)\\right\\|^2$):\n",
    "\n",
    "  - если условие выполнено, то перейти к шагу 9;\n",
    "  - если условие не выполнено, положить $t^k=~\\frac{t^k}{2}$ и перейти к шагу 7.\n",
    "\n",
    "\n",
    "**Шаг 9.** Проверить выполнение условий\n",
    "$\\|x^{k+1}-~x^k\\|<\\varepsilon_2$, $\\left\\|{f(x}^{k+1})-~f(x^k)\\right\\|<\\varepsilon_2$\n",
    "\n",
    "   - если оба условия выполнены при текущем значении $k$ и $k=~k-1$, то расчет окончен, $x^*=~x^{k+1}$;\n",
    "   - если хотя бы одно из условий не выполнено, положить $k=~k+1$ и перейти к шагу 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASJp7D2cQN8x"
   },
   "source": [
    "### Задание 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoTVqmdpQEAY"
   },
   "source": [
    "1) Напишите функцию реализующую алгоритм поиска минимима функции методом\n",
    "\n",
    "    a) градиентного спуска с постоянным шагом. \n",
    "\n",
    "    б) наискорейшего градиентного спуска, т.е. $t_k = \\arg \\min\\limits_{t_k} f (x^k-t_k\\nabla f(x^k))$.\n",
    "    \n",
    "    в) [тяжелого шарика Поляка](http://mech.math.msu.su/~vvb/MasterAI/GradientDescent.html), т.е. $x^{k+1}=x^k−t_k\\nabla f(x^k)+\\beta (x^k−x^{k−1}), 0 < \\beta < 1$.\n",
    "    \n",
    "    г) [градиентного спуска Нестерова](http://mech.math.msu.su/~vvb/MasterAI/GradientDescent.html), т.е.    \n",
    "    $x^{k+1}=x^k−t_k\\nabla f(y^k)$, $y^{k+1}=x^{k+1}+\\beta(x^{k+1}−x^k)$,  $0 < \\beta < 1$.\n",
    "    \n",
    "2) Сравните результаты методов и количество итераций для функций $f_i(x)$, $i=\\overline{1,4}$ из задания 1.1.\n",
    "\n",
    "**Бонусные баллы**\n",
    " - за собственную функцию вычисления $\\nabla f(x^k)$.\n",
    " - проверку методов на [тестовых функциях](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%81%D1%82%D0%BE%D0%B2%D1%8B%D0%B5_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oWc5_Ktoe15g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "import math\n",
    "\n",
    "def grad_func(f, x, h=1e-8):\n",
    "    \"\"\"\n",
    "    Вычисляет градиент функции f в точке x.\n",
    "\n",
    "    Параметры:\n",
    "        f (callable): Функция, для которой вычисляется градиент.\n",
    "        x (array-like): Точка, в которой вычисляется градиент.\n",
    "        h (float): Малое значение для численного дифференцирования.\n",
    "\n",
    "    Возвращает:\n",
    "        numpy.ndarray: Вектор градиента функции f в точке x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_forward = np.copy(x)\n",
    "        x_backward = np.copy(x)\n",
    "        x_forward[i] += h\n",
    "        x_backward[i] -= h\n",
    "        grad[i] = (f(x_forward) - f(x_backward)) / (2 * h)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_const_step(func, init_point, step_size, epsilon1=1e-6, epsilon2=1e-6, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Градиентный спуск с постоянным шагом.\n",
    "\n",
    "    Параметры:\n",
    "        func (callable): Оптимизируемая функция.\n",
    "        init_point (array-like): Начальная точка.\n",
    "        step_size (float): Постоянный шаг.\n",
    "        epsilon1 (float): Точность по норме градиента.\n",
    "        epsilon2 (float): Точность по изменению функции и точек.\n",
    "        max_iterations (int): Максимальное количество итераций.\n",
    "\n",
    "    Возвращает:\n",
    "        tuple: Точка минимума, значение функции в этой точке, количество итераций.\n",
    "    \"\"\"\n",
    "    current_point = np.array(init_point, dtype=float)\n",
    "    iter_count = 0\n",
    "\n",
    "    while iter_count < max_iterations:\n",
    "        grad_current = grad_func(func, current_point)  # Градиент в текущей точке\n",
    "        norm_grad_current = np.linalg.norm(grad_current)  # Норма градиента\n",
    "        if norm_grad_current < epsilon1:\n",
    "            break\n",
    "\n",
    "        next_point = current_point - step_size * grad_current  # Шаг метода\n",
    "\n",
    "        if func(next_point) - func(current_point) >= 0:  # Проверка на уменьшение функции\n",
    "            step_size /= 2\n",
    "            continue\n",
    "\n",
    "        if (np.linalg.norm(next_point - current_point) < epsilon2 and\n",
    "                abs(func(next_point) - func(current_point)) < epsilon2 and iter_count > 0):\n",
    "            current_point = next_point\n",
    "            break\n",
    "\n",
    "        current_point = next_point\n",
    "        iter_count += 1\n",
    "\n",
    "    return current_point, func(current_point), iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_optimal_step(func, init_point, epsilon1=1e-6, epsilon2=1e-6, max_iterations=10000):\n",
    "    \"\"\"\n",
    "    Градиентный спуск с оптимальным шагом.\n",
    "\n",
    "    Параметры:\n",
    "        func (callable): Оптимизируемая функция.\n",
    "        init_point (array-like): Начальная точка.\n",
    "        epsilon1 (float): Точность по норме градиента.\n",
    "        epsilon2 (float): Точность по изменению функции и точек.\n",
    "        max_iterations (int): Максимальное количество итераций.\n",
    "\n",
    "    Возвращает:\n",
    "        tuple: Точка минимума, значение функции в этой точке, количество итераций.\n",
    "    \"\"\"\n",
    "    current_point = np.array(init_point, dtype=float)\n",
    "    iter_count = 0\n",
    "\n",
    "    while iter_count < max_iterations:\n",
    "        grad_current = grad_func(func, current_point)  # Градиент в текущей точке\n",
    "        norm_grad_current = np.linalg.norm(grad_current)  # Норма градиента\n",
    "        if norm_grad_current < epsilon1:\n",
    "            break\n",
    "\n",
    "        def phi(t):\n",
    "            return func(current_point - t * grad_current)  # Функция для нахождения оптимального шага\n",
    "\n",
    "        res = minimize_scalar(phi)  # Минимизируем phi(t)\n",
    "        current_step_size = res.x\n",
    "\n",
    "        next_point = current_point - current_step_size * grad_current  # Шаг метода\n",
    "\n",
    "        if (np.linalg.norm(next_point - current_point) < epsilon2 and\n",
    "                abs(func(next_point) - func(current_point)) < epsilon2 and iter_count > 0):\n",
    "            current_point = next_point\n",
    "            break\n",
    "\n",
    "        current_point = next_point\n",
    "        iter_count += 1\n",
    "\n",
    "    return current_point, func(current_point), iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_heavy_ball(func, init_point, step_size, beta, epsilon1=1e-6, epsilon2=1e-6, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Метод тяжелого шарика Поляка для минимизации функции.\n",
    "\n",
    "    Параметры:\n",
    "        func (callable): Оптимизируемая функция.\n",
    "        init_point (array-like): Начальная точка.\n",
    "        step_size (float): Постоянный шаг градиентного спуска.\n",
    "        beta (float): Коэффициент инерции (от 0 до 1).\n",
    "        epsilon1 (float): Точность для нормы градиента.\n",
    "        epsilon2 (float): Точность для разности между итерациями.\n",
    "        max_iterations (int): Максимальное число итераций.\n",
    "\n",
    "    Возвращает:\n",
    "        tuple: Точка минимума, значение функции в этой точке, количество итераций.\n",
    "    \"\"\"\n",
    "    current_point = np.array(init_point, dtype=float)\n",
    "    prev_point = current_point.copy()\n",
    "    iter_count = 0\n",
    "\n",
    "    while iter_count < max_iterations:\n",
    "        grad_current = grad_func(func, current_point)\n",
    "        norm_grad_current = np.linalg.norm(grad_current)\n",
    "\n",
    "        if norm_grad_current < epsilon1:\n",
    "            break\n",
    "\n",
    "        if iter_count == 0:\n",
    "            next_point = current_point - step_size * grad_current\n",
    "        else:\n",
    "            next_point = current_point - step_size * grad_current + beta * (current_point - prev_point)\n",
    "\n",
    "        if (np.linalg.norm(next_point - current_point) < epsilon2 and\n",
    "                np.abs(func(next_point) - func(current_point)) < epsilon2):\n",
    "            current_point = next_point\n",
    "            break\n",
    "\n",
    "        prev_point = current_point\n",
    "        current_point = next_point\n",
    "        iter_count += 1\n",
    "\n",
    "    return current_point, func(current_point), iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_nesterov(func, init_point, step_size, beta, epsilon1=1e-6, epsilon2=1e-6, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Градиентный спуск Нестерова для минимизации функции.\n",
    "\n",
    "    Параметры:\n",
    "        func (callable): Оптимизируемая функция.\n",
    "        init_point (array-like): Начальная точка.\n",
    "        step_size (float): Постоянный шаг градиентного спуска.\n",
    "        beta (float): Коэффициент инерции (от 0 до 1).\n",
    "        epsilon1 (float): Точность для нормы градиента.\n",
    "        epsilon2 (float): Точность для разности между итерациями.\n",
    "        max_iterations (int): Максимальное число итераций.\n",
    "\n",
    "    Возвращает:\n",
    "        tuple: Точка минимума, значение функции в этой точке, количество итераций.\n",
    "    \"\"\"\n",
    "    current_point = np.array(init_point, dtype=float)\n",
    "    current_func_res = current_point.copy()\n",
    "    iter_count = 0\n",
    "\n",
    "    while iter_count < max_iterations:\n",
    "        grad_fy = grad_func(func, current_func_res)\n",
    "        norm_grad_fy = np.linalg.norm(grad_fy)\n",
    "\n",
    "        if norm_grad_fy < epsilon1:\n",
    "            break\n",
    "\n",
    "        next_point = current_func_res - step_size * grad_fy\n",
    "        next_func_res = next_point + beta * (next_point - current_point)\n",
    "\n",
    "        if (np.linalg.norm(next_point - current_point) < epsilon2 and\n",
    "                np.abs(func(next_point) - func(current_point)) < epsilon2):\n",
    "            current_point = next_point\n",
    "            break\n",
    "\n",
    "        current_point = next_point\n",
    "        current_func_res = next_func_res\n",
    "        iter_count += 1\n",
    "\n",
    "    return current_point, func(current_point), iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1:\n",
      "Градиентный спуск с постоянным шагом:\n",
      "Точка минимума: x = [ 0.50375889 -1.24383822]\n",
      "Значение функции в точке минимума: f(x) = -6.437463946947878\n",
      "Количество итераций: 33261\n",
      "\n",
      "\n",
      "Градиентный спуск с оптимальным шагом:\n",
      "Точка минимума: x = [ 0.50000011 -1.24999989]\n",
      "Значение функции в точке минимума: f(x) = -6.437499999999982\n",
      "Количество итераций: 14\n",
      "\n",
      "\n",
      "Метод тяжелого шарика Поляка:\n",
      "Точка минимума: x = [ 0.50037464 -1.24939305]\n",
      "Значение функции в точке минимума: f(x) = -6.4374996484163844\n",
      "Количество итераций: 4936\n",
      "\n",
      "\n",
      "Градиентный спуск Нестерова:\n",
      "Точка минимума: x = [ 0.50037507 -1.24939236]\n",
      "Значение функции в точке минимума: f(x) = -6.4374996476095125\n",
      "Количество итераций: 4942\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "f2:\n",
      "Градиентный спуск с постоянным шагом:\n",
      "Точка минимума: x = [0.98891929 0.97791673]\n",
      "Значение функции в точке минимума: f(x) = 0.0001229813444966835\n",
      "Количество итераций: 83152\n",
      "\n",
      "\n",
      "Градиентный спуск с оптимальным шагом:\n",
      "Точка минимума: x = [0.99960026 0.99919867]\n",
      "Значение функции в точке минимума: f(x) = 1.6019520959287706e-07\n",
      "Количество итераций: 5072\n",
      "\n",
      "\n",
      "Метод тяжелого шарика Поляка:\n",
      "Точка минимума: x = [0.99888749 0.99777177]\n",
      "Значение функции в точке минимума: f(x) = 1.2396530278354705e-06\n",
      "Количество итераций: 13967\n",
      "\n",
      "\n",
      "Градиентный спуск Нестерова:\n",
      "Точка минимума: x = [0.9988871  0.99777097]\n",
      "Значение функции в точке минимума: f(x) = 1.2405412118318413e-06\n",
      "Количество итераций: 13973\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "f3:\n",
      "Градиентный спуск с постоянным шагом:\n",
      "Точка минимума: x = [-1.49968422  2.24903695]\n",
      "Значение функции в точке минимума: f(x) = -9.999994939164141\n",
      "Количество итераций: 71516\n",
      "\n",
      "\n",
      "Градиентный спуск с оптимальным шагом:\n",
      "Точка минимума: x = [-1.49994799  2.24984109]\n",
      "Значение функции в точке минимума: f(x) = -9.999999862262392\n",
      "Количество итераций: 948\n",
      "\n",
      "\n",
      "Метод тяжелого шарика Поляка:\n",
      "Точка минимума: x = [-1.49997213  2.24991501]\n",
      "Значение функции в точке минимума: f(x) = -9.999999960589612\n",
      "Количество итераций: 7357\n",
      "\n",
      "\n",
      "Градиентный спуск Нестерова:\n",
      "Точка минимума: x = [-1.49997158  2.24991331]\n",
      "Значение функции в точке минимума: f(x) = -9.999999959002118\n",
      "Количество итераций: 7360\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "f4:\n",
      "Градиентный спуск с постоянным шагом:\n",
      "Точка минимума: x = [-0.07582683  0.02736578 -0.01648764]\n",
      "Значение функции в точке минимума: f(x) = 0.00041140650295644343\n",
      "Количество итераций: 39017\n",
      "\n",
      "\n",
      "Градиентный спуск с оптимальным шагом:\n",
      "Точка минимума: x = [-3.49032367e-05  1.29220760e-05 -7.73860614e-06]\n",
      "Значение функции в точке минимума: f(x) = 8.849804837961031e-11\n",
      "Количество итераций: 423\n",
      "\n",
      "\n",
      "Метод тяжелого шарика Поляка:\n",
      "Точка минимума: x = [-0.00757349  0.00273326 -0.00164677]\n",
      "Значение функции в точке минимума: f(x) = 4.104094591691203e-06\n",
      "Количество итераций: 22840\n",
      "\n",
      "\n",
      "Градиентный спуск Нестерова:\n",
      "Точка минимума: x = [-0.00757395  0.00273343 -0.00164687]\n",
      "Значение функции в точке минимума: f(x) = 4.104596003554991e-06\n",
      "Количество итераций: 22842\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "f5:\n",
      "Градиентный спуск с постоянным шагом:\n",
      "Точка минимума: x = [3.35150677 3.35168828]\n",
      "Значение функции в точке минимума: f(x) = 0.44932824747317923\n",
      "Количество итераций: 100000\n",
      "\n",
      "\n",
      "Градиентный спуск с оптимальным шагом:\n",
      "Точка минимума: x = [6.39726011e-06 6.34628601e-06]\n",
      "Значение функции в точке минимума: f(x) = 1.6246292686575806e-12\n",
      "Количество итераций: 17\n",
      "\n",
      "\n",
      "Метод тяжелого шарика Поляка:\n",
      "Точка минимума: x = [0.09147195 0.09147195]\n",
      "Значение функции в точке минимума: f(x) = 0.0003346847273668274\n",
      "Количество итераций: 100000\n",
      "\n",
      "\n",
      "Градиентный спуск Нестерова:\n",
      "Точка минимума: x = [0.09148514 0.09148514]\n",
      "Значение функции в точке минимума: f(x) = 0.00033478121905326525\n",
      "Количество итераций: 100000\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x[0] ** 3 - x[0] * x[1] + x[1] ** 2 - 2 * x[0] + 3 * x[1] - 4\n",
    "\n",
    "def f2(x):\n",
    "    \"\"\"Функция Розенброка\"\"\"\n",
    "    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "def f3(x):\n",
    "    return 10 / (30 * (x[1] - x[0] ** 2) ** 2 + 5 * (1.5 + x[0]) ** 2 + 1)\n",
    "\n",
    "def neg_f3(x):\n",
    "    return -f3(x)\n",
    "\n",
    "def f4(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 2 + 3 * x[2] ** 2 + 4 * x[0] * x[1] - 2 * x[1] * x[2] - 2 * x[0] * x[2]\n",
    "\n",
    "\n",
    "def f5(x):\n",
    "    \"\"\"Функция Матьяса.\n",
    "    min f(0,0) = 0\n",
    "    -10 <= x,y <= 10\"\"\"\n",
    "    return 0.26 * (x[0]**2 + x[1]**2) - 0.48 * x[0] * x[1]\n",
    "\n",
    "initial_points = {\n",
    "    'f1': np.array([0.0, 0.0]),\n",
    "    'f2': np.array([0.0, 0.0]),\n",
    "    'f3': np.array([1.0, 1.0]),\n",
    "    'f4': np.array([0.0, 1.0, 1.0]),\n",
    "    'f5': np.array([3.0, 7.0])\n",
    "}\n",
    "\n",
    "functions = {\n",
    "    'f1': f1,\n",
    "    'f2': f2,\n",
    "    'f3': neg_f3,\n",
    "    'f4': f4,\n",
    "    'f5': f5\n",
    "}\n",
    "\n",
    "step_const = 0.0001\n",
    "step_heavy_ball = 0.0001\n",
    "step_nesterov = 0.0001\n",
    "momentum_heavy_ball = 0.9\n",
    "momentum_nesterov = 0.9\n",
    "\n",
    "for name, func in functions.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    init_point = initial_points[name]\n",
    "\n",
    "    xmin, fmin, iters = gradient_descent_const_step(func, init_point, step_const)\n",
    "    print(f\"Градиентный спуск с постоянным шагом:\")\n",
    "    print(f\"Точка минимума: x = {xmin}\")\n",
    "    print(f\"Значение функции в точке минимума: f(x) = {fmin}\")\n",
    "    print(f\"Количество итераций: {iters}\\n\")\n",
    "\n",
    "    xmin_opt, fmin_opt, iters_opt = gradient_descent_optimal_step(func, init_point)\n",
    "    print(f\"\\nГрадиентный спуск с оптимальным шагом:\")\n",
    "    print(f\"Точка минимума: x = {xmin_opt}\")\n",
    "    print(f\"Значение функции в точке минимума: f(x) = {fmin_opt}\")\n",
    "    print(f\"Количество итераций: {iters_opt}\\n\")\n",
    "\n",
    "    xmin_hb, fmin_hb, iters_hb = gradient_descent_heavy_ball(func, init_point, step_heavy_ball, momentum_heavy_ball)\n",
    "    print(f\"\\nМетод тяжелого шарика Поляка:\")\n",
    "    print(f\"Точка минимума: x = {xmin_hb}\")\n",
    "    print(f\"Значение функции в точке минимума: f(x) = {fmin_hb}\")\n",
    "    print(f\"Количество итераций: {iters_hb}\\n\")\n",
    "\n",
    "    xmin_nest, fmin_nest, iters_nest = gradient_descent_nesterov(func, init_point, step_nesterov, momentum_nesterov)\n",
    "    print(f\"\\nГрадиентный спуск Нестерова:\")\n",
    "    print(f\"Точка минимума: x = {xmin_nest}\")\n",
    "    print(f\"Значение функции в точке минимума: f(x) = {fmin_nest}\")\n",
    "    print(f\"Количество итераций: {iters_nest}\\n\")\n",
    "\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBlMALt4Qdr-"
   },
   "source": [
    "# 1.3 Метод второго порядка (метод Ньютона)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HClLVLYwX1pp"
   },
   "source": [
    "**Шаг 1.** Задать $M, \\varepsilon_1>0,  \\varepsilon_2>0$ и точку начального приближения $x_0$;\n",
    "\n",
    "**Шаг 2.** Положить $k=0$.\n",
    "\n",
    "**Шаг 3.** Вычислить $\\nabla f(x_k)$.\n",
    "\n",
    "**Шаг 4.** Проверить выполнение критерия окончания $||\\nabla f(x_k)||\\leq \\varepsilon _1$:\n",
    "\n",
    "   - если неравествно выполнено, то расчет окончен и $x^*=x^k$;\n",
    "   - в противном случае перейти к шагу 5.\n",
    "\n",
    "\n",
    "**Шаг 5.** Проверить выполнение неравенства $k\\geq M$:\n",
    "\n",
    "   - если неравествно выполнено, то расчет окончен и $x^*=x^k$;\n",
    "   - в противном случае перейти к шагу 6.\n",
    "\n",
    "**Шаг 6.** Вычислить матрицу $H(x^k)$.\n",
    "\n",
    "**Шаг 7.** Вычислить матрицу $H^{-1}(x^k)$.\n",
    "\n",
    "**Шаг 8.** Проверить выполнение условия $H^{-1}(x^k)>0$:\n",
    "\n",
    "   - если $H^{-1}(x_k)>0$, то перейти к шагу 9;\n",
    "   - иначе перейти к шагу 10, положив $d^k=-\\nabla f(x^k)$.\n",
    "\n",
    "**Шаг 9.** Определить $d^k=-H^{-1}(x^k)\\nabla f(x^k)$.\n",
    "\n",
    "**Шаг 10.** Найти точку $x^{k+1}=x^k+t_kd^k$, положив $t_k=1$, если $d^k=-H^{-1}(x^k)\\nabla f(x^k)$, или выбрав $t_k$ из условия $f(x^{k+1})<f(x^k)$, если $d^k=-\\nabla f(x^k)$.\n",
    "\n",
    "**Шаг 11.** Проверить выполнение условий $||x^{k+1}-x^k||<\\varepsilon_1, |f(x^{k+1})-f(x^k)|<\\varepsilon_2$:\n",
    "   - если оба условия выполнены при текущем значение $k$ и $k=k-1$, то расчёт окончен, $x^*=x^{k+1}$;\n",
    "   - в противном случае положить $k=k+1$ и перейти к шагу 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki9CZIEkZZct"
   },
   "source": [
    "### Задание 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JokAJSoqZZ0Q"
   },
   "source": [
    "1) Напишите функцию реализующую алгоритм поиска минимима функции методом Ньютона.  \n",
    "2) С помощью функции найдите минимальное значение функции $$f_1(x)=x_1^3-x_1x_2+x^2_2-2x_1+3x_2-4.$$\n",
    "3) С помощью функции найдите минимальное значение функции Розенброка $$f_2(x)=(1-x_1)^{2}+100(x_2-x_1^{2})^{2}.$$\n",
    "\n",
    "4) С помощью функции найдите максимум функции $$f_3(x)=\\frac{10}{30(x_2-x_1^2)^2+5(1.5+x_1)^2+1}.$$\n",
    "\n",
    "5)  С помощью функции найдите минимальное значение функции  $$f_4(x)=x_1^2+5x_2^2+3x_3^2+4x_1x_2-2x_2x_3-2x_1x_3.$$\n",
    "\n",
    "**Бонусные баллы**\n",
    " - за собственную функцию вычисления $H(x^k)$.\n",
    " - проверку методов на [тестовых функциях](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%81%D1%82%D0%BE%D0%B2%D1%8B%D0%B5_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sQC1Orvye0iB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def grad_func(f, x, h=1e-8):\n",
    "    \"\"\"\n",
    "    Вычисляет градиент функции f в точке x.\n",
    "\n",
    "    Параметры:\n",
    "        f (callable): Функция, для которой вычисляется градиент.\n",
    "        x (array-like): Точка, в которой вычисляется градиент.\n",
    "        h (float): Малое значение для численного дифференцирования.\n",
    "\n",
    "    Возвращает:\n",
    "        numpy.ndarray: Вектор градиента функции f в точке x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_forward = np.copy(x)\n",
    "        x_backward = np.copy(x)\n",
    "        x_forward[i] += h\n",
    "        x_backward[i] -= h\n",
    "        grad[i] = (f(x_forward) - f(x_backward)) / (2 * h)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian(func, point, step_size=1e-5):\n",
    "    \"\"\"Вычисление матрицы Гессе (вторых производных) функции в точке.\"\"\"\n",
    "    dim = len(point)\n",
    "    hessian_matrix = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            point_ijp = np.array(point, copy=True)\n",
    "            point_ijm = np.array(point, copy=True)\n",
    "            point_ipj = np.array(point, copy=True)\n",
    "            point_imj = np.array(point, copy=True)\n",
    "\n",
    "            point_ijp[i] += step_size\n",
    "            point_ijp[j] += step_size\n",
    "            point_ijm[i] += step_size\n",
    "            point_ijm[j] -= step_size\n",
    "            point_ipj[i] -= step_size\n",
    "            point_ipj[j] += step_size\n",
    "            point_imj[i] -= step_size\n",
    "            point_imj[j] -= step_size\n",
    "\n",
    "            hessian_matrix[i, j] = (\n",
    "                func(point_ijp) - func(point_ijm) - func(point_ipj) + func(point_imj)\n",
    "            ) / (4 * step_size**2)\n",
    "    return hessian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(func, initial_guess, grad_tolerance=1e-6, step_tolerance=1e-6, max_iterations=100):\n",
    "    \"\"\"Оптимизация функции методом Ньютона.\"\"\"\n",
    "    current_point = np.array(initial_guess, dtype=float)\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        gradient = grad_func(func, current_point)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm <= grad_tolerance:\n",
    "            break\n",
    "\n",
    "        hessian = compute_hessian(func, current_point)\n",
    "        try:\n",
    "            hessian_inverse = np.linalg.inv(hessian)\n",
    "            search_direction = -np.dot(hessian_inverse, gradient)\n",
    "        except np.linalg.LinAlgError:\n",
    "            search_direction = -gradient\n",
    "\n",
    "        next_point = current_point + search_direction\n",
    "\n",
    "        if (np.linalg.norm(next_point - current_point) < step_tolerance and\n",
    "                abs(func(next_point) - func(current_point)) < step_tolerance and iteration > 0):\n",
    "            current_point = next_point\n",
    "            break\n",
    "\n",
    "        current_point = next_point\n",
    "        iteration += 1\n",
    "\n",
    "    return current_point, func(current_point), iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оптимизации для функции 1:\n",
      "Точка минимума: [ 0.50000014 -1.24999992]\n",
      "Значение функции в минимуме: -6.437499999999975\n",
      "Количество итераций: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x[0] ** 3 - x[0] * x[1] + x[1] ** 2 - 2 * x[0] + 3 * x[1] - 4\n",
    "\n",
    "init_point = [1.0, 1.0]\n",
    "min_point, min_value, iterations = newton_method(f1, init_point)\n",
    "print(\"Результаты оптимизации для функции 1:\")\n",
    "print(f\"Точка минимума: {min_point}\")\n",
    "print(f\"Значение функции в минимуме: {min_value}\")\n",
    "print(f\"Количество итераций: {iterations}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оптимизации для функции Розенброка:\n",
      "Точка минимума: [1. 1.]\n",
      "Значение функции в минимуме: 1.963652286798773e-26\n",
      "Количество итераций: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    \"\"\"Функция Розенброка\"\"\"\n",
    "    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "init_point = [-1.2, 1.0]\n",
    "min_point, min_value, iterations = newton_method(f2, init_point)\n",
    "print(\"Результаты оптимизации для функции Розенброка:\")\n",
    "print(f\"Точка минимума: {min_point}\")\n",
    "print(f\"Значение функции в минимуме: {min_value}\")\n",
    "print(f\"Количество итераций: {iterations}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оптимизации для функции 3:\n",
      "Точка максимума: [ -1.29243027 132.20837832]\n",
      "Значение функции в максимуме: 1.956157361843369e-05\n",
      "Количество итераций: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f3(x):\n",
    "    return 10 / (30 * (x[1] - x[0] ** 2) ** 2 + 5 * (1.5 + x[0]) ** 2 + 1)\n",
    "\n",
    "def neg_f3(x):\n",
    "    return -f3(x)\n",
    "\n",
    "init_point = [-1.25, 2.25]\n",
    "max_point, neg_max_value, iterations = newton_method(neg_f3, init_point)\n",
    "max_value = -neg_max_value\n",
    "print(\"Результаты оптимизации для функции 3:\")\n",
    "print(f\"Точка максимума: {max_point}\")\n",
    "print(f\"Значение функции в максимуме: {max_value}\")\n",
    "print(f\"Количество итераций: {iterations}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оптимизации для функции 4:\n",
      "Точка минимума: [ 1.16036738e-16 -3.83638162e-17  2.38033199e-17]\n",
      "Значение функции в минимуме: 1.0190354644066938e-33\n",
      "Количество итераций: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f4(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 2 + 3 * x[2] ** 2 + 4 * x[0] * x[1] - 2 * x[1] * x[2] - 2 * x[0] * x[2]\n",
    "\n",
    "init_point = [-1.0, -1.0, -1.0]\n",
    "min_point, min_value, iterations = newton_method(f4, init_point)\n",
    "print(\"Результаты оптимизации для функции 4:\")\n",
    "print(f\"Точка минимума: {min_point}\")\n",
    "print(f\"Значение функции в минимуме: {min_value}\")\n",
    "print(f\"Количество итераций: {iterations}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оптимизации для функции 5:\n",
      "Точка минимума: [-1.64177482e-13 -1.55063462e-13]\n",
      "Значение функции в минимуме: 1.0399141476101759e-27\n",
      "Количество итераций: 2\n"
     ]
    }
   ],
   "source": [
    "def f5(x):\n",
    "    \"\"\"Функция Матьяса.\n",
    "    min f(0,0) = 0\n",
    "    -10 <= x,y <= 10\"\"\"\n",
    "    return 0.26 * (x[0]**2 + x[1]**2) - 0.48 * x[0] * x[1]\n",
    "\n",
    "init_point = [4.0, -7.0]\n",
    "min_point, min_value, iterations = newton_method(f5, init_point)\n",
    "print(\"Результаты оптимизации для функции 5:\")\n",
    "print(f\"Точка минимума: {min_point}\")\n",
    "print(f\"Значение функции в минимуме: {min_value}\")\n",
    "print(f\"Количество итераций: {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsAUCmGIZjLZ"
   },
   "source": [
    "##Задание 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHJd7iFeZpLM"
   },
   "source": [
    "1) Выбрать функцию поиска минимума из библиотеки SciPy \n",
    " - опишите все входные и выходные параметры функции, типы данных параметров и методы оптимизации, которые применяются в функции\n",
    " - С помощью функции найдите минимальное значение функции $$f_1(x)=x_1^3-x_1x_2+x^2_2-2x_1+3x_2-4.$$\n",
    " - С помощью функции найдите минимальное значение функции Розенброка $$f_2(x)=(1-x_1)^{2}+100(x_2-x_1^{2})^{2}.$$\n",
    "\n",
    " - С помощью функции найдите максимум функции $$f_3(x)=\\frac{10}{30(x_2-x_1^2)^2+5(1.5+x_1)^2+1}.$$\n",
    "\n",
    " - С помощью функции найдите минимальное значение функции  $$f_4(x)=x_1^2+5x_2^2+3x_3^2+4x_1x_2-2x_2x_3-2x_1x_3.$$\n",
    "\n",
    "2) Сравнить результаты п. 1 с результами выполнения заданий 1.1, 1.2, 1.3. Ответ обосновать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8NSJZq-els0"
   },
   "source": [
    "Ваш ответ здесь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "339EXop8epJ4"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.26\u001b[39m \u001b[39m*\u001b[39m (x[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m x[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m0.48\u001b[39m \u001b[39m*\u001b[39m x[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m x[\u001b[39m1\u001b[39m]\n\u001b[0;32m     25\u001b[0m init_point \u001b[39m=\u001b[39m [\u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m result \u001b[39m=\u001b[39m minimize(f1, init_point)\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mРезультаты оптимизации для функции 1:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mТочка минимума: \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m.\u001b[39mx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KonDI\\projects\\python\\Optimization_methods-1\\.venv\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:726\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    724\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    725\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 726\u001b[0m     res \u001b[39m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m    727\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    728\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    729\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\KonDI\\projects\\python\\Optimization_methods-1\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1371\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m maxiter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1369\u001b[0m     maxiter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x0) \u001b[39m*\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m-> 1371\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m   1372\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[0;32m   1374\u001b[0m f \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun\n\u001b[0;32m   1375\u001b[0m myfprime \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\KonDI\\projects\\python\\Optimization_methods-1\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    284\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    286\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    289\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\KonDI\\projects\\python\\Optimization_methods-1\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:222\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    219\u001b[0m     finite_diff_options[\u001b[39m\"\u001b[39m\u001b[39mas_linear_operator\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m# Initial function evaluation\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    224\u001b[0m \u001b[39m# Initial gradient evaluation\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapped_grad, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ngev \u001b[39m=\u001b[39m _wrapper_grad(\n\u001b[0;32m    226\u001b[0m     grad,\n\u001b[0;32m    227\u001b[0m     fun\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapped_fun,\n\u001b[0;32m    228\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m    229\u001b[0m     finite_diff_options\u001b[39m=\u001b[39mfinite_diff_options\n\u001b[0;32m    230\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\KonDI\\projects\\python\\Optimization_methods-1\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 294\u001b[0m         fx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_fun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n\u001b[0;32m    295\u001b[0m         \u001b[39mif\u001b[39;00m fx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lowest_f:\n\u001b[0;32m    296\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lowest_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\n",
      "File \u001b[1;32mc:\\Users\\KonDI\\projects\\python\\Optimization_methods-1\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m ncalls[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m, in \u001b[0;36mf1\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1\u001b[39m(x):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m x[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mx[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m3\u001b[39m\u001b[39m*\u001b[39mx[\u001b[39m2\u001b[39;49m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m\u001b[39m*\u001b[39mx[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mx[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m8\u001b[39m\u001b[39m*\u001b[39mx[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mx[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m\u001b[39m*\u001b[39mx[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mx[\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def f1(x):\n",
    "    return x[0]**2 + 2*x[1]**2 - 3*x[2]**2 - 6*x[0]*x[1] + 8*x[0]*x[2] - 4*x[1]*x[2]\n",
    "def f2(x):\n",
    "    \"\"\"Функция Розенброка\"\"\"\n",
    "    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "def f3(x):\n",
    "    return 10 / (30 * (x[1] - x[0] ** 2) ** 2 + 5 * (1.5 + x[0]) ** 2 + 1)\n",
    "\n",
    "def neg_f3(x):\n",
    "    return -f3(x)\n",
    "\n",
    "def f4(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 2 + 3 * x[2] ** 2 + 4 * x[0] * x[1] - 2 * x[1] * x[2] - 2 * x[0] * x[2]\n",
    "\n",
    "def f5(x):\n",
    "    \"\"\"Функция Матьяса.\n",
    "    min f(0,0) = 0\n",
    "    -10 <= x,y <= 10\"\"\"\n",
    "    return 0.26 * (x[0]**2 + x[1]**2) - 0.48 * x[0] * x[1]\n",
    "\n",
    "\n",
    "init_point = [0, 0, 0]\n",
    "result = minimize(f1, init_point)\n",
    "print(\"Результаты оптимизации для функции 1:\")\n",
    "print(f\"Точка минимума: {result.x}\")\n",
    "print(f\"Значение функции в минимуме: {result.fun}\")\n",
    "print(f\"Количество итераций: {result.nit}\\n\")\n",
    "\n",
    "init_point = [-1.2, 1.0]\n",
    "result = minimize(f2, init_point)\n",
    "print(\"Результаты оптимизации для функции Розенброка:\")\n",
    "print(f\"Точка минимума: {result.x}\")\n",
    "print(f\"Значение функции в минимуме: {result.fun}\")\n",
    "print(f\"Количество итераций: {result.nit}\\n\")\n",
    "\n",
    "init_point = [-1.25, 2.25]\n",
    "result = minimize(neg_f3, init_point)\n",
    "print(\"Результаты оптимизации для функции 3:\")\n",
    "print(f\"Точка максимума: {result.x}\")\n",
    "print(f\"Значение функции в максимуме: {-result.fun}\")\n",
    "print(f\"Количество итераций: {result.nit}\\n\")\n",
    "\n",
    "init_point = [-1.0, -1.0, -1.0]\n",
    "result = minimize(f4, init_point)\n",
    "print(\"Результаты оптимизации для функции 4:\")\n",
    "print(f\"Точка минимума: {result.x}\")\n",
    "print(f\"Значение функции в минимуме: {result.fun}\")\n",
    "print(f\"Количество итераций: {result.nit}\\n\")\n",
    "\n",
    "init_point = [4.0, -7.0]\n",
    "result = minimize(f5, init_point)\n",
    "print(\"Результаты оптимизации для функции 5:\")\n",
    "print(f\"Точка минимума: {result.x}\")\n",
    "print(f\"Значение функции в минимуме: {result.fun}\")\n",
    "print(f\"Количество итераций: {result.nit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTUhkgIyAR1u"
   },
   "source": [
    "# Часть 2 Решение задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAL_1WNXAZMn"
   },
   "source": [
    "\n",
    "1. Найти и классифицировать матрицу Гессе функции $f\\left(x\\right)=x_{1}^{2} -x_{2}^{2} $\n",
    "2.   Найти и классифицировать матрицу Гессе функции $f\\left(x\\right)=x_{1}^{2} -4x_{1} x_{2}$ \n",
    "3.  Методом первого порядка решить задачу $$f(x)=4(x_1-5)^2+(x_2-6)^2\\to \\min, x^0=(8,9)^T, \\varepsilon_1=\\varepsilon_2=0.1.$$\n",
    "4.   Методом первого порядка решить задачу $$f(x)=-x_1x_2e^{-x_1-x_2}\\to \\min, x^0=(0,1)^T, \\varepsilon_1=\\varepsilon_2=0.1.$$\n",
    "5.   Методом второго порядка решить задачу $$f(x)=x_1^2+x_1x_2+2x_2\\to \\min.$$\n",
    "6.   Методом первого порядка решить задачу $$f(x)=-x_1x_2e^{-x_1-x_2}\\to \\min, x^0=(0,1)^T, \\varepsilon_1=\\varepsilon_2=0.1.$$\n",
    "7.   Методом второго порядка решить задачу $$f(x)=x_1^2+3x_1x_2+2x_2\\to \\min.$$\n",
    "\n",
    "8.   Разделить положительное  число на две части так, чтобы произведение произведения этих частей на их разность было максимальным.\n",
    "9.   На плоскости даны три точки: $x_1$, $x_2$, $x_3$. Найти такую точку $x_0$, чтобы сумма квадратов расстояний от $x_0$ до $x_1$, $x_2$, $x_3$ была минимальной.\n",
    "10. Найти безусловный экстремум функции $f(x)=4x^2_1+3x^2_2-4x_1x_2+x_1$.\n",
    "11. Проверить, является ли точка $x^*=(0,0,0)^T$ точкой безусловного экстремума функции $f(x)=x^2_1+2x^2_2-3x^2_3-6x_1x_2+8x_1x_3-4x_2x_3$.\n",
    "12. Проверить, является ли точка $x^*=(1,1)^T$ точкой безусловного экстремума функции $f(x)=(x_2-x_1^2)^2+(1-x_1)^2+10(x^2-1)^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hASVWpxmC9Am"
   },
   "source": [
    "#Часть 3 Теоретический минимум "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54YYfe8HDCIH"
   },
   "source": [
    "## Теоретический минимум по методам оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgpbgDdtDH7w"
   },
   "source": [
    "1.   В чем заключается задача многомерной безусловной оптимизации?\n",
    "2.   Какие условия называются необходимыми и достаточными условиями безусловного экстремума функции?\n",
    "3.   Что такое точка глобального минимума функции?\n",
    "4.   Что такое точка локального минимума функции?\n",
    "5.   Какая функция называется выпуклой?\n",
    "6.  Что такое матрица Гессе?\n",
    "7.   В чем заключаются принципы построения численных методов поиска безусловного экстремума?\n",
    "8.   Чем характеризуется порядок методов поиска безусловного экстремума?\n",
    "9.   Какой порядок имеет метод градиентного спуска с постоянным шагом?\n",
    "10.  Какой порядок имеет метод наискорейшего градиентного спуска?\n",
    "11.  Какой порядок имеет метод Гаусса-Зейделя?\n",
    "12.  Перечислите методы второго порядка для поиска безусловного экстремума функции нескольких переменных?\n",
    "13.  Какой порядок имеет метод Ньютона?\n",
    "14. Что такое градиент?\n",
    "15.  Какой вид имеет общее правило построения последовательности  ${x^k}$ ? (все переменные пояснить)\n",
    "16.  Каким свойством должны обладать точки последовательности  ${x^k}$ ?\n",
    "17.  Какому условию должно удовлетворять направление спуска  $d^k$  при решении задачи безусловной минимизации?\n",
    "18. Перечислите критерии останова\n",
    "19. Опишите модельную схему решения задач безусловной минимизации\n",
    "20. В чем преимущества метода Ньютона?\n",
    "21. Перечислите недостатки метода Ньютона?\n",
    "22. В чем заключается дополнительное исследование точки $x^k$, полученной методом градиентного спуска с постоянным шагом?\n",
    "23. В чем заключается дополнительное исследование точки $x^k$, полученной методомнаискорейшего градиентного спуска?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNM-2RtIDEhw"
   },
   "source": [
    "## Теоретический минимум python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJkazeO7fLts"
   },
   "source": [
    "1. Опишите реализацию градиента в python\n",
    "2. Как реализована матрица вторых производных в python?\n",
    "3. Опишите реализацию критериев останова?\n",
    "4. Какими свойствами обладают методы из функции, которую Вы выбрали?\n",
    "5. Как проводилось дополнительное исследование точки $x^k$ в python, полученной методомнаискорейшего градиентного спуска?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMplfbrlrreYOD0IxySmp02",
   "provenance": [
    {
     "file_id": "1gIIGni5mI2WxpPuC78GftslUiajm_PA_",
     "timestamp": 1638790679224
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
